{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Leave-word-out\n",
    "\n",
    "This script gets a relevant score for each word, removing the word from test dataset and obtaining the performance metrics for the classification of those customized tweets.\n",
    "\n",
    "The score is calculated as:\n",
    "```\n",
    "custom_dataset_f1_score - original_dataset_f1_score\n",
    "```\n",
    "If the result is positive: removing the word benefits the results, so the word **IS NOT HELPING** in the classification.\n",
    "If the result is negative: removing the word damages the results, so the word **IS HELPING** in the classification.\n",
    "\n",
    "They are being considerated only words that previously were in the top 500 words with greatest absolute attention."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DATASET_FOLDER = 'DS3'\n",
    "CHECKPOINT = 'bert-base-multilingual-uncased'\n",
    "FOLDER = './outputs/DS3/bert-base-multilingual-uncased'\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "# Import words\n",
    "words = pd.read_csv(f'{FOLDER}/words/top_500_words.csv', sep=';').word.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Set-up model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "saved_model = f'{FOLDER}/model/pytorch_model.bin'\n",
    "saved_config = f'{FOLDER}/model/config.json'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    saved_model,\n",
    "    config=saved_config\n",
    ")\n",
    "model.to(device)\n",
    "print(f'Model is designed for {model.num_labels} labels.')\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(instance):\n",
    "    return tokenizer(instance['tweet'], truncation=True)\n",
    "\n",
    "device"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) Load dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load/Calculate performance metrics for dataset\n",
    "test_results = pd.read_csv(f'{FOLDER}/test_results_complete.csv', sep=';')\n",
    "\n",
    "original_test_scores = {\n",
    "    'accuracy': accuracy_score(test_results.expected.values, test_results.got.values),\n",
    "    'precision': precision_score(test_results.expected.values, test_results.got.values, average='weighted'),\n",
    "    'recall': recall_score(test_results.expected.values, test_results.got.values, average='weighted'),\n",
    "    'f1': f1_score(test_results.expected.values, test_results.got.values, average='weighted'),\n",
    "}\n",
    "original_test_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        f'test': f'./datasets/{DATASET_FOLDER}/test_dataset.csv',\n",
    "    }\n",
    ")\n",
    "dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Perform leave-word-out"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric\n",
    "import re\n",
    "\n",
    "words_lwo = {}\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': load_metric('accuracy'),\n",
    "    'precision': load_metric('precision'),\n",
    "    'recall': load_metric('recall'),\n",
    "    'f1': load_metric('f1'),\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "t0 = time.time() \n",
    "\n",
    "for step, word in enumerate(words):\n",
    "    \n",
    "    # Remove word from tweet and tokenize it\n",
    "    tokenized_data = dataset.map(\n",
    "        (\n",
    "            lambda instance: tokenizer(\n",
    "                [ re.sub(fr'\\b{word}\\b', '', tweet) for tweet in instance['tweet'] ],\n",
    "                truncation=True\n",
    "            )\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    tokenized_data = tokenized_data.remove_columns(\n",
    "        ['tweet']\n",
    "    )\n",
    "    tokenized_data = tokenized_data.rename_column('label', 'labels')\n",
    "    tokenized_data.set_format('torch')\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_dataloader = DataLoader(\n",
    "        tokenized_data['test'], batch_size=BATCH_SIZE, collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    eval_metrics = metrics\n",
    "\n",
    "    # Predict for custom test dataset\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "    \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}    \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        for _, metric in eval_metrics.items():\n",
    "            metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "    words_lwo[word] = {\n",
    "        'word': word,\n",
    "        'accuracy': eval_metrics['accuracy'].compute()['accuracy'] - original_test_scores['accuracy'],\n",
    "        'precision': eval_metrics['precision'].compute(average='weighted')['precision'] - original_test_scores['precision'],\n",
    "        'recall': eval_metrics['recall'].compute(average='weighted')['recall'] - original_test_scores['recall'],\n",
    "        'f1': eval_metrics['f1'].compute(average='weighted')['f1'] - original_test_scores['f1'],\n",
    "    }\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        # Report progress.\n",
    "        print('Word {:>3,}  of  {:>3,}. \"{:}\", took: {:}'.format(step, len(words), word, elapsed))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) Save words an leave-word-out scores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lwo_df = pd.DataFrame(words_lwo).T\n",
    "lwo_df.to_csv(f'{FOLDER}/lwo.csv', index=None)\n",
    "lwo_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lwo_df.sort_values('f1', ascending=True)\n",
    "lwo_df[lwo_df.f1 < 0]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}