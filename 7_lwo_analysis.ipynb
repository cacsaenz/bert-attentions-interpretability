{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [14.4, 10.8]\n",
    "plt.rcParams['figure.dpi'] = 200 # 200 e.g. is really fine, but slower\n",
    "\n",
    "RED_COLOR = '#ff6978'\n",
    "BLUE_COLOR = '#2d93ad'\n",
    "\n",
    "TOPS = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "\n",
    "FOLDER = './outputs/DS1/bert-base-multilingual-uncased'\n",
    "CHECKPOINT = 'bert-base-multilingual-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(CHECKPOINT)\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "words_metrics = pd.read_csv(f'{FOLDER}/words_metrics.csv')\n",
    "words_metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lwo_df = pd.read_csv(f'{FOLDER}/lwo.csv')\n",
    "lwo_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "words_df = pd.read_csv(f'{FOLDER}/words/top_500_words.csv', sep=';')\n",
    "words_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "percentages = {}\n",
    "\n",
    "for top in TOPS:\n",
    "    percentages[f'lwo_{top}'] = {\n",
    "        'average_attention': 0.0,\n",
    "        'positive': 0.0,\n",
    "        'negative': 0.0,\n",
    "        'p-relevant': 0.0,\n",
    "        'p-correct': 0.0,\n",
    "    }\n",
    "    \n",
    "    top_absolute = words_df.head(top)\n",
    "    words = top_absolute.word.values\n",
    "    percentages[f'lwo_{top}']['average_attention'] = np.mean(top_absolute.absolute.values)\n",
    "\n",
    "    # Just words that are in top-X words with absolute attention    \n",
    "    subset = lwo_df[lwo_df.word.isin(words)]\n",
    "    positive_words = subset[subset.f1 < 0]\n",
    "    negative_words = subset[subset.f1 > 0]\n",
    "    percentages[f'lwo_{top}']['positive'] = len(positive_words) / top * 100\n",
    "    percentages[f'lwo_{top}']['negative'] = len(negative_words) / top * 100\n",
    "    \n",
    "    tfidf = pd.read_csv(f'{FOLDER}/words/relevant_{top}.csv')\n",
    "    positive_relevant = positive_words[positive_words.word.isin(tfidf.word.values)]\n",
    "    percentages[f'lwo_{top}']['p-relevant'] = len(positive_relevant) / top * 100\n",
    "    \n",
    "    correct = pd.read_csv(f'{FOLDER}/words/correct_{top}.csv')\n",
    "    positive_correct = positive_words[positive_words.word.isin(correct.word.values)]\n",
    "    if top == 250:\n",
    "        positive_correct.to_csv(f'{FOLDER}/words/ploo_piw.csv', index=None)\n",
    "    percentages[f'lwo_{top}']['p-correct'] = len(positive_correct) / top * 100\n",
    "    \n",
    "percentages_df = pd.DataFrame(percentages).T\n",
    "percentages_df.insert(0, column='top', value=percentages_df.index)\n",
    "percentages_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normality test:\n",
    "for metric in ['average_attention', 'positive', 'p-relevant', 'p-correct']:\n",
    "    print('%s: %.4f' % (metric, stats.shapiro(percentages_df[metric].values).pvalue))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get correlations...\n",
    "print(\n",
    "    \"Corr. attention vs positive: %.4f (p=%.4f)\" %\\\n",
    "    stats.spearmanr(percentages_df.average_attention.values, percentages_df.positive.values)\n",
    ")\n",
    "print(\n",
    "    \"Corr. attention vs negative: %.4f (p=%.4f)\" %\\\n",
    "    stats.spearmanr(percentages_df.average_attention.values, percentages_df.negative.values)\n",
    ")\n",
    "print(\n",
    "    \"Corr. attention vs p-relevant: %.4f (p=%.4f)\" %\\\n",
    "    stats.spearmanr(percentages_df.average_attention.values, percentages_df['p-relevant'].values)\n",
    ")\n",
    "print(\n",
    "    \"Corr. attention vs p-correct: %.4f (p=%.4f)\" %\\\n",
    "    stats.spearmanr(percentages_df.average_attention.values, percentages_df['p-correct'].values)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top 30 positive/negative contributing words\n",
    "TOP_N = 20\n",
    "\n",
    "positive_words = lwo_df[lwo_df.f1 < 0].sort_values('f1', ascending=False)\n",
    "negative_words = lwo_df[lwo_df.f1 > 0].sort_values('f1', ascending=False)\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(14.4, 8))\n",
    "fig.subplots_adjust(wspace=0.35)\n",
    "\n",
    "axarr[0].set_xlabel('Words that damages F1 when removed')\n",
    "axarr[1].set_xlabel('Words that improve F1 when removed')\n",
    "\n",
    "positive_words.head(TOP_N).plot(kind='barh', ax=axarr[0], x='word', y='f1', color=BLUE_COLOR, legend=None, xlabel='')\n",
    "negative_words.head(TOP_N).plot(kind='barh', ax=axarr[1], x='word', y='f1', color=RED_COLOR, legend=None, xlabel='')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11804bb1f250a85d0267cdde9a53e916113451a0f88a057835fc51e3493d4141"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}