{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and building dataset\n",
    "\n",
    "We perform the described preprocessing, then we build a DataFrame with preprocessed tweets.\n",
    "Finally we divide the datasets in three parts: `train_set`, `val_set` and `test_set`, and save them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuration presets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PREFIX = './datasets'\n",
    "LABEL_MAPPER = {\n",
    "    'antichina': 0,\n",
    "    'antivacina': 1,\n",
    "    'provacina': 2,\n",
    "}\n",
    "\n",
    "LOWERCASE = True\n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_DATASET_SIZE_BY_CLASS = 2000\n",
    "OUTPUT_FOLDER_NAME = 'DS1' # where dataset is going to be saved, you NEED TO CREATE THE FOLDER FIRST"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "import unidecode\n",
    "import emoji\n",
    "\n",
    "hashtags = [\n",
    "    '#EuVouTomarVacina',\n",
    "    '#VacinaBrasil',\n",
    "    '#VacinaEAmorAoPróximo',\n",
    "    '#VacinaEAmorAoProximo', # without accent\n",
    "    '#VacinaJá',\n",
    "    '#VacinaJa', # without accent\n",
    "    '#VacinaNoBrasil',\n",
    "    '#VacinaParaTodos',\n",
    "    '#VacinasPelaVida',\n",
    "    '#VacinaUrgenteParaTodos',\n",
    "    '#VemVacina',\n",
    "    '#EuNãoVouTomarVacina',\n",
    "    '#EuNaoVouTomarVacina', # withouth accent\n",
    "    '#NãoVouTomarVacina',\n",
    "    '#NaoVouTomarVacina', # without accent\n",
    "    '#VacinaNão',\n",
    "    '#VacinaNao', # without accent\n",
    "    '#VacinaObrigatóriaNão',\n",
    "    '#VacinaObrigatóriaNao', # without accent\n",
    "    '#VacinaObrigatoriaNão', # without accent\n",
    "    '#VacinaObrigatoriaNao', # without accent\n",
    "    '#VachinaNão',\n",
    "    '#VachinaNao', # without accent\n",
    "    '#VachinaNãoPresidente',\n",
    "    '#VachinaNaoPresidente', # without accent\n",
    "    '#VachinaObrigatóriaNão',\n",
    "    '#VachinaObrigatóriaNao', # without accent\n",
    "    '#VachinaObrigatoriaNão', # without accent\n",
    "    '#VachinaObrigatoriaNao', # without accent\n",
    "    '#VacinaChinesaNão',\n",
    "    '#VacinaChinesaNao', # without accent\n",
    "    \n",
    "    # Terms used for scraping\n",
    "    'vacinação',\n",
    "    'vacinacão', # without ç\n",
    "    'vacinaçao', # without accent\n",
    "    'vacinacao', # without ç and accent\n",
    "    'vacina',\n",
    "]\n",
    "\n",
    "\n",
    "def removeNonLatinCharacters(tweet):\n",
    "    return \" \".join(\n",
    "        regex.sub(r'[^\\p{Latin}]', u'', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeNumbers(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(r'[0-9]', '', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeEmojis(tweet):\n",
    "    return \" \".join(\n",
    "        emoji.get_emoji_regexp().sub(r'', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeURLs(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(r\"http\\S+|youtu.be\\S+|\\S+.com.br\\S+|bit.ly\\S+|\\S+.com/\\S+|\\S+.co/\\S+|\\S+.org\\S+|\\S+.br/\\S+|\\S+.es/S+\", \"\", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeHashtags(tweet):\n",
    "    for hashtag in hashtags:\n",
    "        # Here, it could be two variations:\n",
    "        # the hashtag symbol (#) + the HT\n",
    "        # the HT only\n",
    "        ht = re.compile(\n",
    "            re.escape(\n",
    "                hashtag\n",
    "            ),\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        tweet = ht.sub('', tweet)\n",
    "        \n",
    "        ht = re.compile(\n",
    "            re.escape(\n",
    "                hashtag[1:]\n",
    "            ),\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        tweet = ht.sub('', tweet)\n",
    "    \n",
    "    # Finally, replace all empty hashtags symbols (#)\n",
    "    return \" \".join(\n",
    "        re.sub(r'# ', '', tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def removeMentions(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(r\"@\\S+\", \"\", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def isValidTweet(tweet):\n",
    "    without_mentions = \" \".join(re.sub(r\"@\", \"\", tweet).split())\n",
    "    if (\n",
    "        len(without_mentions.split()) < 3 or\n",
    "        not without_mentions or\n",
    "        re.search(\"^\\s*$\", without_mentions)\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocessTweet(tweet, excludingSubstrings=[]):\n",
    "    # Now we have a list of strings that when found on tweet,\n",
    "    # the tweet gets discarded.\n",
    "    for string in excludingSubstrings:\n",
    "        if string in tweet:\n",
    "            return None\n",
    "        \n",
    "    preprocessed = removeNumbers(\n",
    "        removeEmojis(\n",
    "            removeMentions(\n",
    "                removeURLs(\n",
    "                    removeHashtags(tweet)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    if isValidTweet(preprocessed) and preprocessed:\n",
    "        if LOWERCASE:\n",
    "            return preprocessed.lower()\n",
    "        else:\n",
    "            return preprocessed\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is just for testing...\n",
    "antichina = pd.read_csv(f'{PREFIX}/raw_csv/antichina.csv', names=['tweet'])\n",
    "print(antichina['tweet'].values[1435])\n",
    "print(preprocessTweet(antichina['tweet'].values[1435]))\n",
    "print('')\n",
    "\n",
    "antivax = pd.read_csv(f'{PREFIX}/raw_csv/antivacina.csv', names=['tweet'])\n",
    "print(antivax['tweet'].values[37])\n",
    "print(preprocessTweet(antivax['tweet'].values[37]))\n",
    "print('')\n",
    "\n",
    "provax = pd.read_csv(f'{PREFIX}/raw_csv/provacina.csv', names=['tweet'])\n",
    "print(provax['tweet'].values[45])\n",
    "print(preprocessTweet(provax['tweet'].values[45], ['VaiPassar']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get hashtags distribution (just for later analysis)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "antichina = pd.read_csv(f'{PREFIX}/raw_csv/antichina.csv', names=['tweet'])\n",
    "found_hashtags = {}\n",
    "for tweet in antichina['tweet'].values:\n",
    "    preprocessed = preprocessTweet(tweet)\n",
    "    if preprocessed == None:\n",
    "        continue\n",
    "    for ht in [tag.strip(\"#\") for tag in preprocessed.split() if tag.startswith(\"#\")]:\n",
    "        if ht not in found_hashtags:\n",
    "            found_hashtags[ht] = {\n",
    "                'ht': ht,\n",
    "                'count': 0,\n",
    "            }\n",
    "        found_hashtags[ht]['count'] += 1\n",
    "ht_df = pd.DataFrame(found_hashtags).T\n",
    "ht_df.to_csv(f'{PREFIX}/{OUTPUT_FOLDER_NAME}/antichina_hts.csv', sep=';', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset building"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load and build dataset\n",
    "print('Building dataset...\\n')\n",
    "\n",
    "labels = []\n",
    "tweets = []\n",
    "\n",
    "for label in [\n",
    "    'antichina',\n",
    "    'antivacina',\n",
    "    'provacina'\n",
    "]:\n",
    "    print(f'Preprocessing {label}...')\n",
    "    print(' With excluding words:')\n",
    "    \n",
    "    group_df = pd.read_csv(f'{PREFIX}/raw_csv/{label}.csv', names=['tweet'])\n",
    "    \n",
    "    for tweet in group_df['tweet'].values:\n",
    "        try:\n",
    "            preprocessed_tweet = preprocessTweet(tweet)\n",
    "        except:\n",
    "            print(f'Failed with {tweet}')\n",
    "            continue\n",
    "\n",
    "        if preprocessed_tweet:\n",
    "            tweets.append(preprocessed_tweet)\n",
    "            labels.append(LABEL_MAPPER[label])\n",
    "    \n",
    "    print('Finished.\\n')\n",
    "    \n",
    "dataset = pd.DataFrame({\n",
    "    'tweet': tweets,\n",
    "    'label': labels,\n",
    "})\n",
    "dataset.to_csv(f'{PREFIX}/{OUTPUT_FOLDER_NAME}/complete_dataset.csv', sep=';', columns=['tweet', 'label'], index=False)\n",
    "dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Complete dataset metrics...')\n",
    "print('# Antichina:', len(dataset[dataset['label'] == 0]))\n",
    "print('# Antivacina:', len(dataset[dataset['label'] == 1]))\n",
    "print('# Provacina:', len(dataset[dataset['label'] == 2]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now build a with the same size of elements of each class\n",
    "sample = pd.concat([\n",
    "    dataset[dataset['label'] == 0].sample(random_state=RANDOM_SEED, n=OUTPUT_DATASET_SIZE_BY_CLASS).reset_index(drop=True),\n",
    "    dataset[dataset['label'] == 1].sample(random_state=RANDOM_SEED, n=OUTPUT_DATASET_SIZE_BY_CLASS).reset_index(drop=True),\n",
    "    dataset[dataset['label'] == 2].sample(random_state=RANDOM_SEED, n=OUTPUT_DATASET_SIZE_BY_CLASS).reset_index(drop=True),\n",
    "]).reset_index(drop=True)\n",
    "sample.to_csv(f'{PREFIX}/{OUTPUT_FOLDER_NAME}/dataset.csv', sep=';', columns=['tweet', 'label'], index=False)\n",
    "sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Sample dataset metrics...')\n",
    "print('# Antichina:', len(sample[sample['label'] == 0]))\n",
    "print('# Antivacina:', len(sample[sample['label'] == 1]))\n",
    "print('# Provacina:', len(sample[sample['label'] == 2]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    sample,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=sample[['label']]\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=train_df[['label']]\n",
    ")\n",
    "\n",
    "# Report the number of tweets.\n",
    "print('Number of training sentences: {:,}'.format(train_df.shape[0]))\n",
    "print('  # Antichina:', len(train_df[train_df['label'] == 0]))\n",
    "print('  # Antivacina:', len(train_df[train_df['label'] == 1]))\n",
    "print('  # Provacina:', len(train_df[train_df['label'] == 2]))\n",
    "print('')\n",
    "\n",
    "print('Number of validation sentences: {:,}'.format(val_df.shape[0]))\n",
    "print('  # Antichina:', len(val_df[val_df['label'] == 0]))\n",
    "print('  # Antivacina:', len(val_df[val_df['label'] == 1]))\n",
    "print('  # Provacina:', len(val_df[val_df['label'] == 2]))\n",
    "print('')\n",
    "\n",
    "print('Number of testing sentences: {:,}'.format(test_df.shape[0]))\n",
    "print('  # Antichina:', len(test_df[test_df['label'] == 0]))\n",
    "print('  # Antivacina:', len(test_df[test_df['label'] == 1]))\n",
    "print('  # Provacina:', len(test_df[test_df['label'] == 2]))\n",
    "\n",
    "# Save train/val/test dataframes\n",
    "train_df.to_csv(f'{PREFIX}/{OUTPUT_FOLDER_NAME}/train_dataset.csv', index=None)\n",
    "val_df.to_csv(f'{PREFIX}/{OUTPUT_FOLDER_NAME}/val_dataset.csv', index=None)\n",
    "test_df.to_csv(f'{PREFIX}/{OUTPUT_FOLDER_NAME}/test_dataset.csv', index=None)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}